"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
–ó–∞–ø—É—Å–∫: python test_llm.py
"""

import asyncio
import os
from dotenv import load_dotenv
from bot.llm import get_llm

load_dotenv()


async def test_basic_question():
    """–¢–µ—Å—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞"""
    print("=" * 60)
    print("–¢–ï–°–¢ 1: –ü—Ä–æ—Å—Ç–æ–π –≤–æ–ø—Ä–æ—Å")
    print("=" * 60)
    
    llm = get_llm()
    question = "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –≤–∏–∑—ã D7?"
    
    print(f"\n‚ùì –í–æ–ø—Ä–æ—Å: {question}")
    print("\n‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...\n")
    
    response = await llm.generate_answer(question)
    
    print(f"‚úÖ –û—Ç–≤–µ—Ç:\n{response.answer}")
    print(f"\nüìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {response.confidence:.2%}")
    print(f"üîç Reasoning: {response.reasoning}")


async def test_with_context():
    """–¢–µ—Å—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ 2: –í–æ–ø—Ä–æ—Å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º")
    print("=" * 60)
    
    llm = get_llm()
    question = "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –≤–∏–∑–∞ D7?"
    
    # –ò–º–∏—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    context = [
        (
            "–ö–∞–∫–∏–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –≤–∏–∑—ã D7?",
            "–ö–æ–Ω—Å—É–ª—å—Å–∫–∏–π —Å–±–æ—Ä - ‚Ç¨83. –¢–∞–∫–∂–µ –Ω—É–∂–Ω–æ —É—á–µ—Å—Ç—å –ø–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (~‚Ç¨200), –∞–ø–æ—Å—Ç–∏–ª—å (‚Ç¨10-30 –∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç)."
        ),
        (
            "–°–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–Ω–∏–º–∞–µ—Ç –ø–æ–ª—É—á–µ–Ω–∏–µ D7?",
            "–û–±—ã—á–Ω–æ 2-3 –º–µ—Å—è—Ü–∞ –æ—Ç –ø–æ–¥–∞—á–∏ –¥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∏–∑—ã."
        )
    ]
    
    print(f"\n‚ùì –í–æ–ø—Ä–æ—Å: {question}")
    print(f"\nüìö –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ({len(context)} –∑–∞–ø–∏—Å–µ–π)")
    print("\n‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...\n")
    
    response = await llm.generate_answer(question, context)
    
    print(f"‚úÖ –û—Ç–≤–µ—Ç:\n{response.answer}")
    print(f"\nüìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {response.confidence:.2%}")
    print(f"üîç Reasoning: {response.reasoning}")


async def test_embedding():
    """–¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    print("=" * 60)
    
    llm = get_llm()
    text = "–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –≤–∏–∑—É D7 –≤ –ü–æ—Ä—Ç—É–≥–∞–ª–∏—é?"
    
    print(f"\nüìù –¢–µ–∫—Å—Ç: {text}")
    print("\n‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —ç–º–±–µ–¥–¥–∏–Ω–≥...\n")
    
    embedding = await llm.generate_embedding(text)
    
    print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
    print(f"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(embedding)}")
    print(f"üî¢ –ü–µ—Ä–≤—ã–µ 5 –∑–Ω–∞—á–µ–Ω–∏–π: {embedding[:5]}")


async def test_confidence_levels():
    """–¢–µ—Å—Ç —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ 4: –£—Ä–æ–≤–Ω–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏")
    print("=" * 60)
    
    llm = get_llm()
    
    questions = [
        ("–ö–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞ –ü–æ—Ä—Ç—É–≥–∞–ª–∏–∏?", "–ü—Ä–æ—Å—Ç–æ–π —Ñ–∞–∫—Ç"),
        ("–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è D7?", "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å"),
        ("–ú–æ–≥—É –ª–∏ —è –ø–æ–ª—É—á–∏—Ç—å –≤–∏–∑—É –µ—Å–ª–∏ —É –º–µ–Ω—è —Å—É–¥–∏–º–æ—Å—Ç—å –∑–∞ –Ω–µ—É–ø–ª–∞—Ç—É –Ω–∞–ª–æ–≥–æ–≤ 10 –ª–µ—Ç –Ω–∞–∑–∞–¥?", "–°–ª–æ–∂–Ω—ã–π –∫–µ–π—Å")
    ]
    
    for question, description in questions:
        print(f"\n{'‚îÄ' * 60}")
        print(f"üìå {description}")
        print(f"‚ùì {question}")
        
        response = await llm.generate_answer(question)
        
        print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {response.confidence:.2%}")
        
        threshold = float(os.getenv("CONFIDENCE_THRESHOLD", "0.7"))
        if response.confidence >= threshold:
            print("‚úÖ –ë—É–¥–µ—Ç –æ—Ç–≤–µ—á–µ–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
        else:
            print("‚ö†Ô∏è  –ë—É–¥–µ—Ç —ç—Å–∫–∞–ª–∏—Ä–æ–≤–∞–Ω–æ –∞–¥–º–∏–Ω–∞–º")


async def test_multilingual():
    """–¢–µ—Å—Ç –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö"""
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ 5: –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å")
    print("=" * 60)
    
    llm = get_llm()
    
    questions = [
        ("Quais s√£o os requisitos para o visto D7?", "–ü–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π"),
        ("What are the requirements for D7 visa?", "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π"),
        ("–ö–∞–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è –≤–∏–∑—ã D7?", "–†—É—Å—Å–∫–∏–π")
    ]
    
    for question, lang in questions:
        print(f"\n{'‚îÄ' * 60}")
        print(f"üåç –Ø–∑—ã–∫: {lang}")
        print(f"‚ùì {question}")
        print("\n‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...\n")
        
        response = await llm.generate_answer(question)
        
        print(f"‚úÖ –û—Ç–≤–µ—Ç:\n{response.answer[:200]}...")
        print(f"\nüìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {response.confidence:.2%}")


async def main():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤"""
    print("\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï LLM –ü–†–û–í–ê–ô–î–ï–†–ê")
    print(f"üîß Provider: {os.getenv('LLM_PROVIDER', 'ollama')}")
    print(f"ü§ñ Model: {os.getenv('OLLAMA_MODEL', 'N/A')}")
    print(f"üéØ Threshold: {os.getenv('CONFIDENCE_THRESHOLD', '0.7')}")
    
    try:
        await test_basic_question()
        await test_with_context()
        await test_embedding()
        await test_confidence_levels()
        await test_multilingual()
        
        print("\n" + "=" * 60)
        print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ó–ê–í–ï–†–®–ï–ù–´")
        print("=" * 60)
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        print("\n–ü—Ä–æ–≤–µ—Ä—å:")
        print("  - –ó–∞–ø—É—â–µ–Ω –ª–∏ Ollama: docker-compose ps")
        print("  - –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å: docker exec <ollama-container> ollama list")
        print("  - –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω .env —Ñ–∞–π–ª")


if __name__ == "__main__":
    asyncio.run(main())